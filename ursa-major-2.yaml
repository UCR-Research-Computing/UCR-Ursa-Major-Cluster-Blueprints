
# +-++-++-+ +-++-++-++-++-++-++-++-+ +-++-++-++-++-++-++-++-++-+
# |U||C||R| |R||e||s||e||a||r||c||h| |C||o||m||p||u||t||i||n||g|
# +-++-++-+ +-++-++-++-++-++-++-++-+ +-++-++-++-++-++-++-++-++-+
#        +-++-++-++-+ +-++-++-++-++-+ +-++-++-+                 
#        |U||r||s||a| |M||a||j||o||r| |H||P||C|                 
#        +-++-++-++-+ +-++-++-++-++-+ +-++-++-+                 

---

blueprint_name: ursa-major

vars:
  project_id: ucr-research-computing
  deployment_name: ursa-major
  region: us-central1
  zone: us-central1-a

# Documentation for each of the modules used below can be found at
# https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/main/modules/README.md

deployment_groups:
- group: primary
  modules:
  - id: ursa-major-cluster-network
    source: modules/network/pre-existing-vpc

  ## Filesystems
  - id: appsfs
    source: modules/file-system/filestore
    use: [ursa-major-cluster-network]
    settings:
      local_mount: /sw

  - id: homefs
    source: modules/file-system/pre-existing-network-storage
    use: [ursa-major-cluster-network]
    settings:
      filestore_tier: BASIC_SSD
      size_gb: 2560
      local_mount: /home

  #- id: scratchfs
   # source: community/modules/file-system/DDN-EXAScaler
    #use: [network1]
    #settings:
     # local_mount: /scratch

  ## Install Scripts
  - id: spack
    source: community/modules/scripts/spack-install
    settings:
      install_dir: /sw/spack
      spack_url: https://github.com/spack/spack
      spack_ref: v0.18.0
      log_file: /var/log/spack.log
      configs:
      - type: single-config
        scope: defaults
        content: "config:build_stage:/sw/spack/spack-stage"
      - type: file
        scope: defaults
        content: |
          modules:
            default:
              tcl:
                hash_length: 0
                all:
                  conflict:
                    - '{name}'
                projections:
                  all: '{name}/{version}-{compiler.name}-{compiler.version}'
      compilers:
      - gcc@10.3.0 target=x86_64
      packages:
      - intel-mpi@2018.4.274%gcc@10.3.0
      #- gromacs@2021.2 %gcc@10.3.0 ^intel-mpi@2018.4.274
      # Uncomment and update the name and path to add a shared or personal Spack
      # cache location to speed up future deployments.
      # spack_cache_url:
      #- mirror_name: gcs_cache
      #  mirror_url: gs://ursa-major-spack-install-cache/

  - id: spack-startup
    source: modules/scripts/startup-script
    settings:
      runners:
      - type: shell
        source: modules/startup-script/examples/install_ansible.sh
        destination: install_ansible.sh
      - $(spack.install_spack_deps_runner)
      - $(spack.install_spack_runner)


# Partitions
  - id: intel_smp_partition
    source: community/modules/compute/SchedMD-slurm-on-gcp-partition
    use:
    - ursa-major-cluster-network
    - homefs
    - appsfs
    settings:
      partition_name: intel_smp
      max_node_count: 16
      enable_placement: false
      exclusive: false
      image_hyperthreads: true
      machine_type: c2-standard-30

  - id: intel_mpi_partition
    source: community/modules/compute/SchedMD-slurm-on-gcp-partition
    use:
    - ursa-major-cluster-network
    - homefs
    - appsfs
    settings:
      partition_name: intel_mpi
      max_node_count: 125
      enable_placement: true
      exclusive: true
      image_hyperthreads: true
      machine_type: c2-standard-4

  - id: amd_mpi_partition
    source: community/modules/compute/SchedMD-slurm-on-gcp-partition
    use:
    - ursa-major-cluster-network
    - homefs
    - appsfs
    #- data-bucket
    settings:
      partition_name: amd_mpi
      max_node_count: 125
      enable_placement: true
      exclusive: true
      image_hyperthreads: true
      machine_type: c2d-standard-4

  - id: amd_smp_partition
    source: community/modules/compute/SchedMD-slurm-on-gcp-partition
    use:
    - ursa-major-cluster-network
    - homefs
    - appsfs
    #- data-bucket
    settings:
      partition_name: amd_smp
      max_node_count: 16
      enable_placement: false
      exclusive: false
      image_hyperthreads: true
      machine_type: c2d-highcpu-32

  - id: data_transfer_partition
    source: community/modules/compute/SchedMD-slurm-on-gcp-partition
    use:
    - ursa-major-cluster-network
    - homefs
    - appsfs
    #- data-bucket
    settings:
      partition_name: data_transfer
      max_node_count: 4
      enable_placement: false
      exclusive: true
      image_hyperthreads: true
      machine_type: c2d-standard-112

  - id: highmem_partition
    source: community/modules/compute/SchedMD-slurm-on-gcp-partition
    use:
    - ursa-major-cluster-network
    - homefs
    - appsfs
    #- data-bucket
    settings:
      partition_name: highmem
      max_node_count: 12
      enable_placement: false
      exclusive: true
      image_hyperthreads: true
      machine_type: m1-ultramem-40
  
  - id: gpu_t4_partition
    source: community/modules/compute/SchedMD-slurm-on-gcp-partition
    use:
    - ursa-major-cluster-network
    - homefs
    - appsfs
    #- data-bucket
    settings:
      partition_name: gpu_t4
      max_node_count: 10
      enable_placement: false
      exclusive: true
      image_hyperthreads: true
      machine_type: n1-standard-4
      gpu_type: nvidia-tesla-t4
      gpu_count: 1 

  - id: debug_partition
    source: community/modules/compute/SchedMD-slurm-on-gcp-partition
    use:
    - ursa-major-cluster-network
    - homefs
    - appsfs
    #- data-bucket
    settings:
      partition_name: debug
      max_node_count: 125
      enable_placement: false
      exclusive: false
      image_hyperthreads: true
      machine_type: e2-standard-4

  - id: main_partition
    source: community/modules/compute/SchedMD-slurm-on-gcp-partition
    use:
    - ursa-major-cluster-network
    - homefs
    - appsfs
    #- data-bucket
    settings:
      partition_name: main
      max_node_count: 31
      enable_placement: false
      exclusive: false
      image_hyperthreads: true
      machine_type: c2-standard-16

  - id: slurm_controller
    source: community/modules/scheduler/SchedMD-slurm-on-gcp-controller
    use:
    - ursa-major-cluster-network
    - homefs
    - appsfs
    - main_partition
    - debug_partition
    - intel_mpi_partition
    - intel_smp_partition
    - amd_mpi_partition
    - amd_smp_partition
    - gpu_t4_partition
    - highmem_partition
    - data_transfer_partition
    settings:
      login_node_count: 1
      suspend_time: 60
      cluster_name: ursa-major

  - id: slurm_login
    source: community/modules/scheduler/SchedMD-slurm-on-gcp-login-node
    use:
    - ursa-major-cluster-network
    - homefs
    - appsfs
    - slurm_controller
    - spack-startup
    settings:
      cluster_name: ursa-major
      login_machine_type: c2-standard-8
